{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c1cb4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-01 18:58:41,487: INFO: data_transformation: Shape of X_train_scaled: (1902, 15)]\n",
      "[2026-01-01 18:58:41,487: INFO: data_transformation: Shape of X_test_scaled: (476, 15)]\n",
      "[2026-01-01 18:58:41,793: INFO: data_transformation: Shape of y_train: (1902,)]\n",
      "[2026-01-01 18:58:41,794: INFO: data_transformation: Shape of y_test: (476,)]\n",
      "Model: LogisticRegression\n",
      "Accuracy Score: 93.28\n",
      "Roc/ Auc Score: 88.66\n",
      "Best Score (roc/auc): 92.53\n",
      "Best Params: {'C': 10, 'max_iter': 1000, 'solver': 'liblinear'}\n",
      "Best Estimator: LogisticRegression(C=10, max_iter=1000, solver='liblinear')\n",
      "Model: RandomForestClassifier\n",
      "Accuracy Score: 92.86\n",
      "Roc/ Auc Score: 87.26\n",
      "Best Score (roc/auc): 92.08\n",
      "Best Params: {'criterion': 'entropy', 'max_depth': 10, 'max_features': 'log2', 'min_samples_leaf': 4, 'n_estimators': 50}\n",
      "Best Estimator: RandomForestClassifier(criterion='entropy', max_depth=10, max_features='log2',\n",
      "                       min_samples_leaf=4, n_estimators=50)\n",
      "Model: GradientBoostingClassifier\n",
      "Accuracy Score: 92.65\n",
      "Roc/ Auc Score: 87.22\n",
      "Best Score (roc/auc): 91.89\n",
      "Best Params: {'learning_rate': 0.1, 'max_depth': 3, 'min_samples_split': 2, 'n_estimators': 50}\n",
      "Best Estimator: GradientBoostingClassifier(n_estimators=50)\n",
      "Model: SVC\n",
      "Accuracy Score: 93.49\n",
      "Roc/ Auc Score: 84.53\n",
      "Best Score (roc/auc): 91.47\n",
      "Best Params: {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "Best Estimator: SVC(C=1, gamma=0.001, probability=True)\n",
      "Model: BaggingClassifier\n",
      "Accuracy Score: 92.86\n",
      "Roc/ Auc Score: 85.96\n",
      "Best Score (roc/auc): 90.02\n",
      "Best Params: {'max_features': 0.6, 'max_samples': 1.0, 'n_estimators': 50}\n",
      "Best Estimator: BaggingClassifier(max_features=0.6, max_samples=1.0, n_estimators=50)\n",
      "Model: KNeighborsClassifier\n",
      "Accuracy Score: 93.28\n",
      "Roc/ Auc Score: 85.02\n",
      "Best Score (roc/auc): 89.87\n",
      "Best Params: {'metric': 'minkowski', 'n_neighbors': 13, 'weights': 'uniform'}\n",
      "Best Estimator: KNeighborsClassifier(n_neighbors=13)\n",
      "Model: XGBClassifier\n",
      "Accuracy Score: 93.28\n",
      "Roc/ Auc Score: 85.06\n",
      "Best Score (roc/auc): 92.44\n",
      "Best Params: {'colsample_bytree': 0.6, 'gamma': 5, 'max_depth': 4, 'min_child_weight': 1, 'subsample': 1.0}\n",
      "Best Estimator: XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=0.6, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              feature_weights=None, gamma=5, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=4,\n",
      "              max_leaves=None, min_child_weight=1, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, ...)\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# data transformation\n",
    "from src.data.data_transformation import DataTransformation\n",
    "\n",
    "# models for hyper-parameter tuning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier, BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# metrics\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "# GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# helpers\n",
    "from helpers.config import load_config\n",
    "from helpers.logger import logger\n",
    "\n",
    "# pandas, numpy and typing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Any, Dict, Optional, List\n",
    "\n",
    "\n",
    "class ModelTrainer:\n",
    "    \"\"\"Module to train and perform grid-search for optimal hyper-parameters for best model.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: dict, data: DataTransformation | None = None):\n",
    "        \"\"\"Initialize ModelTrainer class.\n",
    "        \n",
    "        Args:\n",
    "            config (dict): Configuration file consisting of features, targets, path, ect.\n",
    "            data (DataTransformation): module with scaled training/testing features and targets.\n",
    "        \"\"\"\n",
    "        self.config = config or load_config()\n",
    "        self.data = data or DataTransformation(self.config)\n",
    "        self.results = []\n",
    "        \n",
    "    def load_models_and_params(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Load in hyper-parameters from sklearn models and models with the parameters.\n",
    "        \n",
    "        Returns:\n",
    "            params (List[Dict, Any]): model name with parameters for GridSearchCV.\n",
    "            models (List[Dict, Any]): models with hyper parameters.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # load in models with parameters\n",
    "            \n",
    "            params = {\n",
    "                \"LogisticRegression_params\": {\n",
    "                    \"C\": [0.001, 0.01, 0.1,1,10],\n",
    "                    \"solver\": [\"liblinear\", \"saga\"], \n",
    "                    \"max_iter\": [1000, 5000, 10000]\n",
    "                },\n",
    "                'GradientBoostingClassifier_params': {\n",
    "                    'n_estimators': [50,100,200],\n",
    "                    'learning_rate': [1,0.5,0.25,0.1,0.05,0.01],\n",
    "                    'max_depth': [3,4,5],\n",
    "                    'min_samples_split': [2,5,10],\n",
    "                },\n",
    "                'SVC_params': {\n",
    "                    'C': [0.1,1,10,100,1000],\n",
    "                    'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "                    'kernel': ['rbf']\n",
    "                },\n",
    "                'RandomForestClassifier_params': {\n",
    "                    'n_estimators': [50,100,200],\n",
    "                    'max_depth': [None,10,20],\n",
    "                    'min_samples_leaf':[1,2,4],\n",
    "                    'max_features': ['sqrt','log2',None],\n",
    "                    'criterion':['gini', 'entropy']\n",
    "                },\n",
    "                'BaggingClassifier_params': {\n",
    "                    'n_estimators': [50,100,200],\n",
    "                    'max_samples' : [1.0,0.8,0.6],\n",
    "                    'max_features': [1.0,0.8,0.6]\n",
    "                },\n",
    "                \n",
    "                'KNeighborsClassifier_params': {\n",
    "                    'n_neighbors' : [5,7,9,11,13,15],\n",
    "                    'weights' : ['uniform','distance'],\n",
    "                    'metric' : ['minkowski','euclidean','manhattan']\n",
    "                },\n",
    "                'XGBClassifier_params': {\n",
    "                    'min_child_weight': [1,5,10],\n",
    "                    'gamma': [0.5,1,1.5,2,5],\n",
    "                    'subsample': [0.6,0.8,1.0],\n",
    "                    'colsample_bytree': [0.6,0.8,1.0],\n",
    "                    'max_depth': [3,4,5]\n",
    "        },\n",
    "            }\n",
    "    \n",
    "\n",
    "\n",
    "            models = {\n",
    "                \"LogisticRegression\": (LogisticRegression(), params['LogisticRegression_params']),\n",
    "                \"RandomForestClassifier\": (RandomForestClassifier(), params['RandomForestClassifier_params']),\n",
    "                \"GradientBoostingClassifier\": (GradientBoostingClassifier(), params['GradientBoostingClassifier_params']),\n",
    "                \"SVC\": (SVC(probability=True), params['SVC_params']),\n",
    "                \"BaggingClassifier\": (BaggingClassifier(), params['BaggingClassifier_params']),\n",
    "                \"KNeighborsClassifier\":(KNeighborsClassifier(),params['KNeighborsClassifier_params']),\n",
    "                \"XGBClassifier\":(XGBClassifier(objective=\"binary:logistic\"),params['XGBClassifier_params'])\n",
    "}\n",
    "            return params,models\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading in models and parameters: {e}\")\n",
    "            return {[]}, {[]}\n",
    "    \n",
    "    def get_best_model(self) -> None:\n",
    "        \"\"\"Perform GridSearch on models\"\"\"\n",
    "        try:\n",
    "            # load in X_train_scaled, X_test_scaled, y_train, y_test\n",
    "            \n",
    "            X_train_scaled, X_test_scaled = self.data.split_transform_features()\n",
    "            y_train, y_test = self.data.split_targets()\n",
    "            \n",
    "            # params and models\n",
    "            params, models = self.load_models_and_params()\n",
    "            \n",
    "            \n",
    "            # GridSearchCV and logging through MlFlow.\n",
    "            for model_name, (model, param) in models.items():\n",
    "      \n",
    "                grid_search = GridSearchCV(model, param, cv=4, scoring=\"roc_auc\", n_jobs=-1)\n",
    "                grid_search.fit(X_train_scaled, y_train)\n",
    "                \n",
    "                # predictions and predicted probability.\n",
    "                \n",
    "                y_pred = grid_search.predict(X_test_scaled)\n",
    "                y_pred_prob = grid_search.predict_proba(X_test_scaled)[:,1]\n",
    "                \n",
    "                # model name\n",
    "                print(f\"Model: {model_name}\")\n",
    "                \n",
    "                # accuracy\n",
    "                \n",
    "                acc = accuracy_score(y_test, y_pred)\n",
    "                print(f\"Accuracy Score: {acc*100:.2f}\")\n",
    "                \n",
    "                # roc/auc score\n",
    "                \n",
    "                roc = roc_auc_score(y_test, y_pred_prob)\n",
    "                print(f\"Roc/ Auc Score: {roc*100:.2f}\")\n",
    "                \n",
    "                # best score from grid-search\n",
    "                best_score = grid_search.best_score_\n",
    "                print(f\"Best Score (roc/auc): {best_score*100:.2f}\")\n",
    "                \n",
    "                # best params from grid_search\n",
    "                \n",
    "                best_params = grid_search.best_params_\n",
    "                print(f\"Best Params: {best_params}\")\n",
    "                \n",
    "                # best estimator\n",
    "                \n",
    "                best_estimator = grid_search.best_estimator_\n",
    "                print(f\"Best Estimator: {best_estimator}\")\n",
    "\n",
    "\n",
    "                \n",
    "                    \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Could not run models: {e}\")\n",
    "            return None\n",
    "                \n",
    "\n",
    "obj = ModelTrainer(config=load_config())\n",
    "results = obj.get_best_model()     \n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a9d05f",
   "metadata": {},
   "source": [
    "- You go by the scoring metric with the grid-search. Since GridSearchCV is exhaustive, you use a small cv (Cross-Validation) size.\n",
    "- Logistic Regression performed the best."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
